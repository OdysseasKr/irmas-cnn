{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments using the mel-frequency cepstrum (MFC) as feature\n",
    "\n",
    "This notebook shows you how to train a CNN using the mel-frequency cepstrum of the IRMAS tracks. You can find the IRMAS dataset [here](http://www.mtg.upf.edu/download/datasets/irmas/)\n",
    "\n",
    "Every track in the trainset is 3 seconds long. Using the Librosa function call\n",
    "``` python\n",
    "librosa.feature.melspectrogram()\n",
    "```\n",
    "we get a (128, 130) array. Similarly, for the trainset, we split every track to 3-second tracks and calculate their MFC in the same way. This is easily done by the data preprocessing class included in the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset preprocesso\n",
    "from DatasetPreprocess import DatasetPreprocessor\n",
    "\n",
    "dp = DatasetPreprocessor('mel')\n",
    "dp.generateTrain() # This will create a .h5 file containing the trainset\n",
    "dp.generateTest() # This will create a .h5 file containing the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repository contains two different CNN architectures. One of them is inspired by the YOLO architecture that is used for object detection in pictures. The architecture contains several layers of ReLU-activated Convolutional layers and one fully-connected (dense) layer. Take a look at the paper [here](https://pjreddie.com/media/files/papers/yolo.pdf).\n",
    "\n",
    " Let's import the model and start training. For the sake of simplicity we are going to work with only 3 instruments: Flute, Electric Guitar and Piano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models import yololike_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open dataset\n",
    "keys = ['flu', 'gel', 'pia'] # The keys of the 4 instruments to be used\n",
    "dataset = h5py.File('train_mel_normalized.h5', 'r')\n",
    "vector_size = dataset.attrs['vector_size']\n",
    "num_of_labels = len(keys)\n",
    "num_of_tracks = sum([dataset[x].shape[0] for x in keys])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two arrays for our examples. One of them should contain the features and the other the labels in one-hot represention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1932, 128, 130)\n",
      "(1932, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training and testing\n",
    "features = np.zeros((num_of_tracks, vector_size[0], vector_size[1]), dtype=np.float32)\n",
    "labels = np.zeros((num_of_tracks, len(keys)), dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "for ki, k in enumerate(keys):\n",
    "\tfeatures[i:i + len(dataset[k])] = np.nan_to_num(dataset[k])\n",
    "\tlabels[i:i + len(dataset[k]), ki] = 1\n",
    "\ti += len(dataset[k])\n",
    "    \n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and evaluate the model on the trainset to see how it performs on one-instrument tracks. We will later do the same for multi-instrument songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1738, 128, 130)\n",
      "(194, 128, 130)\n"
     ]
    }
   ],
   "source": [
    "# Split trainset to train and evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(features, labels, test_size=0.1, random_state=1337)\n",
    "print(X_train.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to add and train our model. We are using the new Tensorflow 1.0 high level API with tf.layers and tf.estimator. It resembles Keras. More information [here](https://www.tensorflow.org/programmers_guide/#high_level_apis).\n",
    "\n",
    "This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odysseas/Documents/irmas-cnn/models/yolo-mel-flu,gel,pia\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6b4368a50>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/home/odysseas/Documents/irmas-cnn/models/yolo-mel-flu,gel,pia', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "('conv10', TensorShape([Dimension(10), Dimension(4), Dimension(4), Dimension(1024)]))\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/odysseas/Documents/irmas-cnn/models/yolo-mel-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.10139, step = 0\n",
      "INFO:tensorflow:global_step/sec: 10.0869\n",
      "INFO:tensorflow:loss = 0.9105255, step = 100 (9.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1519\n",
      "INFO:tensorflow:loss = 1.00789, step = 200 (9.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2175\n",
      "INFO:tensorflow:loss = 0.9359663, step = 300 (9.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2109\n",
      "INFO:tensorflow:loss = 0.89697266, step = 400 (9.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1191\n",
      "INFO:tensorflow:loss = 1.1326998, step = 500 (9.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0134\n",
      "INFO:tensorflow:loss = 0.5923571, step = 600 (9.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.096\n",
      "INFO:tensorflow:loss = 0.7662384, step = 700 (9.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1055\n",
      "INFO:tensorflow:loss = 0.6942809, step = 800 (9.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0904\n",
      "INFO:tensorflow:loss = 0.3467595, step = 900 (9.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1147\n",
      "INFO:tensorflow:loss = 0.84814787, step = 1000 (9.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1107\n",
      "INFO:tensorflow:loss = 0.41229638, step = 1100 (9.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1189\n",
      "INFO:tensorflow:loss = 0.73795414, step = 1200 (9.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1238\n",
      "INFO:tensorflow:loss = 0.35937873, step = 1300 (9.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.98745\n",
      "INFO:tensorflow:loss = 0.5775286, step = 1400 (10.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.94244\n",
      "INFO:tensorflow:loss = 0.41707683, step = 1500 (10.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1568\n",
      "INFO:tensorflow:loss = 0.4082974, step = 1600 (9.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1008\n",
      "INFO:tensorflow:loss = 0.4620512, step = 1700 (9.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1365\n",
      "INFO:tensorflow:loss = 0.43580008, step = 1800 (9.865 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1226\n",
      "INFO:tensorflow:loss = 0.36722168, step = 1900 (9.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1282\n",
      "INFO:tensorflow:loss = 0.45741794, step = 2000 (9.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1281\n",
      "INFO:tensorflow:loss = 0.5755684, step = 2100 (9.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1308\n",
      "INFO:tensorflow:loss = 0.3596284, step = 2200 (9.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1216\n",
      "INFO:tensorflow:loss = 0.4708193, step = 2300 (9.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1218\n",
      "INFO:tensorflow:loss = 0.016739039, step = 2400 (9.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1119\n",
      "INFO:tensorflow:loss = 0.10668626, step = 2500 (9.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1206\n",
      "INFO:tensorflow:loss = 0.44535828, step = 2600 (9.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0361\n",
      "INFO:tensorflow:loss = 0.13546437, step = 2700 (9.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1261\n",
      "INFO:tensorflow:loss = 0.064327694, step = 2800 (9.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1152\n",
      "INFO:tensorflow:loss = 0.14582053, step = 2900 (9.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1162\n",
      "INFO:tensorflow:loss = 0.2555115, step = 3000 (9.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1182\n",
      "INFO:tensorflow:loss = 0.1464606, step = 3100 (9.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1225\n",
      "INFO:tensorflow:loss = 0.006582138, step = 3200 (9.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1166\n",
      "INFO:tensorflow:loss = 0.06473999, step = 3300 (9.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1143\n",
      "INFO:tensorflow:loss = 0.22744851, step = 3400 (9.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1166\n",
      "INFO:tensorflow:loss = 0.027258238, step = 3500 (9.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1178\n",
      "INFO:tensorflow:loss = 0.01093646, step = 3600 (9.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1158\n",
      "INFO:tensorflow:loss = 0.010363169, step = 3700 (9.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1185\n",
      "INFO:tensorflow:loss = 0.2663493, step = 3800 (9.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95202\n",
      "INFO:tensorflow:loss = 0.0005119135, step = 3900 (10.048 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /home/odysseas/Documents/irmas-cnn/models/yolo-mel-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.043179493.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7ff62a694390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = os.getcwd() + '/models/yolo-mel-{}'.format(','.join(keys))\n",
    "print(saved_model_path)\n",
    "\n",
    "classifier = tf.estimator.Estimator(model_fn=yololike_model, model_dir=saved_model_path)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x=X_train, y=y_train, batch_size=10, num_epochs=None, shuffle=True)\n",
    "classifier.train(input_fn=train_input_fn, steps=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 4000 steps, our model is trained. Let's see how it went by evaluating on the trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "('conv10', TensorShape([Dimension(None), Dimension(4), Dimension(4), Dimension(1024)]))\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-14:06:34\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/odysseas/Documents/irmas-cnn/models/yolo-mel-flu,gel,pia/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-14:06:36\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.77835053, global_step = 4000, loss = 0.8866062\n",
      "{'loss': 0.8866062, 'global_step': 4000, 'accuracy': 0.77835053}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x=X_eval,y=y_eval,num_epochs=1,shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not seem too bad for a first try...\n",
    "\n",
    "Now let's try to detect the primary instrument of a song using the same network. Time to use our testset.\n",
    "\n",
    "First, we need to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = h5py.File(\"test_mel_normalized.h5\", 'r')\n",
    "instruments = dataset.attrs['instruments']\n",
    "vector_size = dataset.attrs['vector_size']\n",
    "\n",
    "# Prepare data for training and testing\n",
    "features = np.array(dataset['features'])\n",
    "labels = np.array(dataset['labels'])\n",
    "\n",
    "# Keep only samples with a primary instrument being one of the 'gac', 'gel', 'tru', 'vio'\n",
    "key_indices = [np.where(instruments == x)[0][0] for x in keys]\n",
    "example_indices = np.array([])\n",
    "for ind in key_indices:\n",
    "    tmp = np.argwhere(labels[:,ind] == True).flatten()\n",
    "    example_indices = np.union1d(example_indices, tmp).astype(np.int32)\n",
    "\n",
    "features = features[example_indices].astype(np.float32)\n",
    "example_indices = [[x for i in key_indices] for x in example_indices]\n",
    "labels = labels[example_indices, key_indices].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the classifier in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "('conv10', TensorShape([Dimension(None), Dimension(4), Dimension(4), Dimension(1024)]))\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-14:07:12\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/odysseas/Documents/irmas-cnn/models/yolo-mel-flu,gel,pia/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-14:07:17\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.58196133, global_step = 4000, loss = 1.8562407\n",
      "{'loss': 1.8562407, 'global_step': 4000, 'accuracy': 0.58196133}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x=features, y=labels, num_epochs=1, shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaand the accuracy dropps drastically. It is clear that classifying solo instruments is much easier than detecting instruments in a track. Maybe this can be imporved by using a different trainset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
