{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments using the various handpicked features\n",
    "\n",
    "This notebook shows you how to train a CNN various features of the IRMAS tracks. You can find the IRMAS dataset [here](http://www.mtg.upf.edu/download/datasets/irmas/)\n",
    "\n",
    "The features used are:\n",
    "\n",
    "- Spectral Centroid\n",
    "- Spectral Bandwidth\n",
    "- Spectral Rolloff\n",
    "- Zero-crossing rate\n",
    "- RMSE\n",
    "- MFCC\n",
    "\n",
    "Librosa was used to extract all of these features.\n",
    "\n",
    "Every track in the trainset is 3 seconds long. Using the Librosa function calls\n",
    "``` python\n",
    "librosa.feature.spectral_centroid()\n",
    "librosa.feature.spectral_bandwidth()\n",
    "librosa.feature.spectral_rolloff()\n",
    "librosa.feature.zero_crossing_rate()\n",
    "librosa.feature.rmse()\n",
    "librosa.feature.mfcc()\n",
    "```\n",
    "we get a (25, 130) array. Similarly, for the trainset, we split the features of every track so that their dimensions match the (25, 130) arrays from the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset preprocesso\n",
    "from DatasetPreprocess import DatasetPreprocessor\n",
    "\n",
    "dp = DatasetPreprocessor('handpicked')\n",
    "dp.generateTrain() # This will create a .h5 file containing the trainset\n",
    "dp.generateTest() # This will create a .h5 file containing the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repository contains two different CNN architectures. For this experiment we are using the VGG-16 model.The architecture contains several layers of ReLU-activated Convolutional layers and three fully-connected (dense) layers. Take a look at the paper [here](https://arxiv.org/abs/1409.1556). \n",
    "\n",
    "Let's import the model and start training. For the sake of simplicity we are going to work with only 3 instruments: Flute, Electric Guitar and Piano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models import vgg16_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open dataset\n",
    "keys = ['flu', 'gel', 'pia'] # The keys of the 4 instruments to be used\n",
    "dataset = h5py.File('train_handpicked_normalized.h5', 'r')\n",
    "vector_size = dataset.attrs['vector_size']\n",
    "num_of_labels = len(keys)\n",
    "num_of_tracks = sum([dataset[x].shape[0] for x in keys])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two arrays for our examples. One of them should contain the features and the other the labels in one-hot represention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1932, 25, 130)\n",
      "(1932, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training and testing\n",
    "features = np.zeros((num_of_tracks, vector_size[0], vector_size[1]), dtype=np.float32)\n",
    "labels = np.zeros((num_of_tracks, len(keys)), dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "for ki, k in enumerate(keys):\n",
    "\tfeatures[i:i + len(dataset[k])] = np.nan_to_num(dataset[k])\n",
    "\tlabels[i:i + len(dataset[k]), ki] = 1\n",
    "\ti += len(dataset[k])\n",
    "    \n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and evaluate the model on the trainset to see how it performs on one-instrument tracks. We will later do the same for multi-instrument songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1738, 25, 130)\n",
      "(194, 25, 130)\n"
     ]
    }
   ],
   "source": [
    "# Split trainset to train and evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(features, labels, test_size=0.1, random_state=1337)\n",
    "print(X_train.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to add our model. We are using the new Tensorflow 1.0 high level API with tf.layers and tf.estimator. It resembles Keras. More information [here](https://www.tensorflow.org/programmers_guide/#high_level_apis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f62588c1590>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0985693, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.9466\n",
      "INFO:tensorflow:loss = 0.9129079, step = 101 (51.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94571\n",
      "INFO:tensorflow:loss = 0.8941623, step = 201 (51.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95596\n",
      "INFO:tensorflow:loss = 1.0235238, step = 301 (51.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.96188\n",
      "INFO:tensorflow:loss = 1.0066966, step = 401 (50.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95433\n",
      "INFO:tensorflow:loss = 1.0275682, step = 501 (51.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95037\n",
      "INFO:tensorflow:loss = 0.76160055, step = 601 (51.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94852\n",
      "INFO:tensorflow:loss = 0.54379934, step = 701 (51.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9526\n",
      "INFO:tensorflow:loss = 0.86759406, step = 801 (51.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9479\n",
      "INFO:tensorflow:loss = 0.66428137, step = 901 (51.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94165\n",
      "INFO:tensorflow:loss = 0.9614124, step = 1001 (51.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94836\n",
      "INFO:tensorflow:loss = 1.0566967, step = 1101 (51.325 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1170 into /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.90367\n",
      "INFO:tensorflow:loss = 1.1207349, step = 1201 (52.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94254\n",
      "INFO:tensorflow:loss = 0.6792326, step = 1301 (51.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94858\n",
      "INFO:tensorflow:loss = 0.82994145, step = 1401 (51.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9542\n",
      "INFO:tensorflow:loss = 0.5633747, step = 1501 (51.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.96047\n",
      "INFO:tensorflow:loss = 0.7255721, step = 1601 (51.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.96153\n",
      "INFO:tensorflow:loss = 0.61308175, step = 1701 (50.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95899\n",
      "INFO:tensorflow:loss = 0.75581074, step = 1801 (51.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9391\n",
      "INFO:tensorflow:loss = 0.9827625, step = 1901 (51.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9264\n",
      "INFO:tensorflow:loss = 0.46930686, step = 2001 (51.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92411\n",
      "INFO:tensorflow:loss = 0.62507814, step = 2101 (51.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92121\n",
      "INFO:tensorflow:loss = 0.89669496, step = 2201 (52.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92037\n",
      "INFO:tensorflow:loss = 0.7254784, step = 2301 (52.073 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2334 into /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.92524\n",
      "INFO:tensorflow:loss = 0.48855945, step = 2401 (51.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95157\n",
      "INFO:tensorflow:loss = 0.45758936, step = 2501 (51.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94618\n",
      "INFO:tensorflow:loss = 0.42507204, step = 2601 (51.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94151\n",
      "INFO:tensorflow:loss = 0.37863165, step = 2701 (51.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94127\n",
      "INFO:tensorflow:loss = 0.509479, step = 2801 (51.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.93512\n",
      "INFO:tensorflow:loss = 0.99410784, step = 2901 (51.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94718\n",
      "INFO:tensorflow:loss = 0.6655021, step = 3001 (51.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95349\n",
      "INFO:tensorflow:loss = 1.0111538, step = 3101 (51.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94296\n",
      "INFO:tensorflow:loss = 0.39106652, step = 3201 (51.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9464\n",
      "INFO:tensorflow:loss = 0.29253808, step = 3301 (51.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9504\n",
      "INFO:tensorflow:loss = 0.4590837, step = 3401 (51.272 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3501 into /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.9322\n",
      "INFO:tensorflow:loss = 0.29053363, step = 3501 (51.754 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95701\n",
      "INFO:tensorflow:loss = 0.525478, step = 3601 (51.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94816\n",
      "INFO:tensorflow:loss = 0.69655323, step = 3701 (51.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95043\n",
      "INFO:tensorflow:loss = 0.53260416, step = 3801 (51.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95581\n",
      "INFO:tensorflow:loss = 0.5182826, step = 3901 (51.130 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6027737.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f62588c1390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = os.getcwd() + '/models/vgg-handpicked-{}'.format(','.join(keys))\n",
    "print(saved_model_path)\n",
    "\n",
    "classifier = tf.estimator.Estimator(model_fn=vgg16_model, model_dir=saved_model_path)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x=X_train, y=y_train, batch_size=10, num_epochs=None, shuffle=True)\n",
    "classifier.train(input_fn=train_input_fn, steps=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model took a long time to be trained on CPU as it struggled to fit in my GPU memory. However, after 4000 steps, our model is trained. Let's see how it went by evaluating on the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-16:12:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-16:12:50\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.7061856, global_step = 4000, loss = 0.7089133\n",
      "{'loss': 0.7089133, 'global_step': 4000, 'accuracy': 0.7061856}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x=X_eval,y=y_eval,num_epochs=1,shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This does not seem too bad for a first try...\n",
    "\n",
    "Now let's try to detect the primary instrument of a song using the same network. Time to use our testset.\n",
    "\n",
    "First, we need to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = h5py.File(\"test_handpicked_normalized.h5\", 'r')\n",
    "instruments = dataset.attrs['instruments']\n",
    "vector_size = dataset.attrs['vector_size']\n",
    "\n",
    "# Prepare data for training and testing\n",
    "features = np.array(dataset['features'])\n",
    "labels = np.array(dataset['labels'])\n",
    "\n",
    "# Keep only samples with a primary instrument being one of the 'gac', 'gel', 'tru', 'vio'\n",
    "key_indices = [np.where(instruments == x)[0][0] for x in keys]\n",
    "example_indices = np.array([])\n",
    "for ind in key_indices:\n",
    "    tmp = np.argwhere(labels[:,ind] == True).flatten()\n",
    "    example_indices = np.union1d(example_indices, tmp).astype(np.int32)\n",
    "\n",
    "features = features[example_indices].astype(np.float32)\n",
    "example_indices = [[x for i in key_indices] for x in example_indices]\n",
    "labels = labels[example_indices, key_indices].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the classifier in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-16:13:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/odysseas/Documents/irmas-cnn/models/vgg-handpicked-flu,gel,pia/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-16:13:45\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.62705797, global_step = 4000, loss = 1.2474327\n",
      "{'loss': 1.2474327, 'global_step': 4000, 'accuracy': 0.62705797}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x=features, y=labels, num_epochs=1, shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the accuracy is higher than in the experiment using the YOLO-like model. It is clear that classifying solo instruments is much easier than detecting instruments in a track."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
